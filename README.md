# 컴퓨터 비전을 이용한 프로젝트 정리
- SRGAN code execution(8월3일)
- source : https://github.com/leftthomas/SRGAN

# 영상 관련 용어 정리
- 코덱(Codec) : 영상이나 음성 신호를 디지털 신호로 변환하거나 반대로 변환하는 기능을 수행하는 기술, 코더(Coder)와 디코더(Decoder)의 합성어
- MPEG : Moving Picture Experts Group, ISO 및 IEC 산하에서 비디오와 오디오 등 멀티미디어의 표준과 압축기술을 개발을 담당하는 소규모 그룹 
- ex) MPEG1, MPEG2, MPEG4, MPEG7, MPEG21
- MPEG1 : 최초의 비디오와 오디오 표준 압출기술, CD와 같은 매체에서 동영상을 담기 위해 사용, 최대 전송률 약 1.5Mbps이고 최대 해상도가 352x288
- MPEG2 : 디지털 방송이나 DVD와 같은 동영상 압축에 사용되는 손실 압축기술, 오래된 기술이지만 전송률 4~100Mbps, 아직도 디지털 방송에서 사용, Full HD 해상도까지 구현
- MP3 : 음악과 음성 데이터를 압축하는 기술, MP3는 MPEG3 가 아니라 MPEG1의 레이어3을 말하는 것
- MPEG4 : 현재 우리의 일상에서 흔히 사용되는 포맷, 유투브와 같이 인터넷상에 업로드되는 동영상은 대부분, 줄여서 MP4로 표현, 양방향 멀티미디어(동영상, 화상)구현하기 위한 압축 기술, 64Kbps급의 낮은 속도, 높은 압축률을 구현, 고화질 영상의 뛰어난 압축 효율성을 보이는 H.264 코덱과 함께 사용
- MPEG7 : 정보검색을 위한 목적으로 사용되는 압축 표준, 멀티미디어 정보, 콘텐츠를 제작 및 전송, 저장, 유통, 검색 분야에서 사용, 필요한 정보를 찾고자 할 때 데이터베이스에서 쉽게 탐지하고 추출, 
- MPEG21 : 네트워크나 장치에 있는 멀티미디어 자원을 효율적으로 이용하기 위해 개발된 방식, 주로 전자상거래를 위한 목적으로 사용, 사용자가 콘텐츠를 클릭 및 입력하고 표현되는 상호작용 방식 

- 디인터레이스 (Deinterlacing) : 인터레이스(비월주사)방식의 영상을 프로그레시브(순차주사로 바꾸는 과정)
- 인터레이스 (Interlace) : tv가 최초 발명됐을때 부족한 하드웨어 성능으로 인하여 사람들이 원하는 대로 초당 60회 이상 화명을 스캔할 수 없었다. 그래서 하나의 화면을 가로로 마디마디로 자른뒤 처음은 홀수 번째 줄만 스캔을 하고 그 다음은 짝수 번재 줄만 스캔하는 방식으로 초당 60장의 영상을 구현하였는데 이것이 인터레이스다. 스캔된 결과물 하나하나를 필드라고 하며 짝수줄만 스캔한 것을 탑 필드, 홀 수 줄만 스캔한 것을 바텀 필드로 구분, 탑필듣와 바텀필드를 합치면 하나의 프레임이 된다. 프로그레시브 영상은 하나의 화면이 하나의 프레임을 이룬다.

#  SRGAN 
- train.py 실행하여 100 epoch 학습
1. github 코드 다운
$ git clone https://github.com/leftthomas/SRGAN.git SRGAN

$ cd data
$ sudo apt install unzip
$ wget http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip
$ unzip DIV2K_train_HR.zip


- python test_image.py --image_name=sz.jpg


window.__APOLLO_STATE__={"$ROOT_QUERY.velog_config({\"username\":\"hyun-wle\"})":{"title":"LoE","logo_image":null,"__typename":"VelogConfig"},"ROOT_QUERY":{"velog_config({\"username\":\"hyun-wle\"})":{"type":"id","generated":true,"id":"$ROOT_QUERY.velog_config({\"username\":\"hyun-wle\"})","typename":"VelogConfig"},"post({\"url_slug\":\"SRGAN-따라하기\",\"username\":\"hyun-wle\"})":{"type":"id","generated":false,"id":"Post:810b29de-7c97-4013-be91-a9738a1e0eff","typename":"Post"},"auth":{"type":"id","generated":false,"id":"User:fe0f9328-93e4-4e28-ad8e-d27da8fb10ea","typename":"User"}},"Post:810b29de-7c97-4013-be91-a9738a1e0eff":{"id":"810b29de-7c97-4013-be91-a9738a1e0eff","title":"SRGAN 따라하기","released_at":"2022-05-11T12:20:15.013Z","updated_at":"2022-08-03T03:57:31.638Z","tags":{"type":"json","json":[]},"body":"https://github.com/leftthomas/SRGAN\n\nSuper Resolution으로 유명한 SRGAN 모델\n\n# SRGAN을 사용해보자\n## 0. 환경설정\n- ubuntu 18.04 LTS\n- git, conda, pip3, pytorch가 설치된 환경.\n인터넷에 설치법이 쉽게 나와있으니 참고하도록 하자\n\n## 1. 코드 다운로드 \n- github에서 코드를 다운받는다.\n`git clone https://github.com/leftthomas/SRGAN.git SRGAN`\n\n```bash\nroot@7310e7ec2d72:~# git clone https://github.com/leftthomas/SRGAN.git SRGAN\nCloning into 'SRGAN'...\nremote: Enumerating objects: 1064, done.\nremote: Total 1064 (delta 0), reused 0 (delta 0), pack-reused 1064\nReceiving objects: 100% (1064/1064), 32.17 MiB | 8.22 MiB/s, done.\nResolving deltas: 100% (675/675), done.\nroot@7310e7ec2d72:~#\n```\n- SRGAN 폴더로 이동\n`cd SRGAN`\n- 폴더 내용물 확인\n`ls`\n```bash\nroot@7310e7ec2d72:~/SRGAN# ls\nLICENSE    benchmark_results  data_utils.py  images   model.py      statistics         test_image.py  train.py\nREADME.md  data               epochs         loss.py  pytorch_ssim  test_benchmark.py  test_video.py  training_results\n```\nREADME.md 파일에 사용법이 있을 것이다.\n- 텍스트파일 내용물 확인\n`cat README.md`\n```bash\n# SRGAN\nA PyTorch implementation of SRGAN based on CVPR 2017 paper\n[Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network](https://arxiv.org/abs/1609.04802).\n\n## Requirements\n- [Anaconda](https://www.anaconda.com/download/)\n- PyTorch\n...\n```\n내용물이 길고 마크다운 언어로 작성된 듯하다. \ngithub 페이지에서 직접 확인하자.\n\nhttps://github.com/leftthomas/SRGAN\n웹브라우저에서 접속\n페이지 하단에 아래와 같은 내용을 확인 할 수 있다.\n![](https://velog.velcdn.com/images/hyun-wle/post/cb14b237-f1e0-439a-bad6-522744806aed/image.png)\n\n\n## 2. Train\n모든 머신러닝 모델은 학습이 필요하다.\nREADME.md의 Usage 부분에서 확인한 대로 SRGAN 폴더에서\n`python train.py`\n\n```bash\nroot@7310e7ec2d72:~/SRGAN# python train.py\nTraceback (most recent call last):\n  File \"train.py\", line 32, in \u003cmodule>\n    train_set = TrainDatasetFromFolder('data/DIV2K_train_HR', crop_size=CROP_SIZE, upscale_factor=UPSCALE_FACTOR)\n  File \"/opt/ml/SRGAN/data_utils.py\", line 44, in __init__\n    self.image_filenames = [join(dataset_dir, x) for x in listdir(dataset_dir) if is_image_file(x)]\nFileNotFoundError: [Errno 2] No such file or directory: 'data/DIV2K_train_HR'\n```\n에러가 뜨면서 실행되지 않는다.\n\n마지막 줄에 에러 원인이 나와있다.\n```\nFileNotFoundError: [Errno 2] No such file or directory: 'data/DIV2K_train_HR'\n```\n`'data/DIV2K_train_HR'` 경로를 찾을 수 없다고 한다. \nREADME.md 파일을 보니 dataset은 따로 다운로드 해야한다.\n![](https://velog.velcdn.com/images/hyun-wle/post/5e5880c9-78e0-464e-b378-e50ea57b6128/image.png)\n\nhere라고 적힌 링크를 따라가니 baidu 클라우드로 연결된다.\n다운로드하기 복잡하므로 다른 dataset을 이용하자.\n\nREADME.md 파일엔 VOC2012 dataset을 사용한다고 적혀있는데, 코드상에선 DIV2K dataset을 활용했다.\n\n자주 쓰이는 image dataset은 나중에 따로 정리하도록 하고, 이번에는 DIV2K Dataset을 이용해보자.\n\nhttps://data.vision.ee.ethz.ch/cvl/DIV2K/\n\n위 링크에서 다운로드 가능하다. \n![](https://velog.velcdn.com/images/hyun-wle/post/4f8010bc-4c87-474e-b49b-86ab99cb3806/image.png)\n페이지 최 하단부에 다운로드 링크가 있다.\n\n학습과 검증을 위해선 Train Dataset과 Validation Dataset이 필요하다\n또한 SR 학습을 위해 Low Resolution High Resolution 파일이 모두 필요하다.\n\n그러나 우리의 코드에는 해상도를 변환해주는 코드가 있으므로 최하단의 두개의 High Resolution images 파일만 다운로드 받아준다.\n\n각각 3.4GB, 0.4GB 정도이다.\n\n터미널 환경에서도 다운로드 할 수 있다.\n먼저 SRGAN/data 폴더로 이동하자.\n`cd data`\n그리고 다음 명령어로 다운받는다.\n`wget http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip`\n`wget http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_valid_HR.zip`\n```\nroot@7310e7ec2d72:~/SRGAN/data# wget http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip\n--2022-05-11 12:50:22--  http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip\nResolving data.vision.ee.ethz.ch (data.vision.ee.ethz.ch)... 129.132.52.178, 3001:67a:10ec:37b2::108\nConnecting to data.vision.ee.ethz.ch (data.vision.ee.ethz.ch)|110.172.52.228|:80... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip [following]\n--2022-05-11 12:50:32--  https://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip\nConnecting to data.vision.ee.ethz.ch (data.vision.ee.ethz.ch)|110.172.52.228|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3530603713 (3.3G) [application/zip]\nSaving to: ‘DIV2K_train_HR.zip’\n\nDIV2K_train_HR.zip             37%[=================>                                ]   1.24G  11.2MB/s    eta 3m 18s\n```\n\nwget이 설치되지 않은 경우\n`sudo apt install wget` 을 먼저 실행한다.\n```\nroot@7310e7ec2d72:~/SRGAN/data# apt install wget\nReading package lists... Done\nBuilding dependency tree\nReading state information... Done\nwget is already the newest version (1.19.4-1ubuntu2.2).\n0 upgraded, 0 newly installed, 0 to remove and 36 not upgraded.\n```\n\n두 파일을 모두 다운로드 했으면, 압축을 해제시켜줘야 한다.\n`ls` 명령어로 다운받은 파일을 확인한다.\n```\nroot@7310e7ec2d72:~/SRGAN/data# ls\nDIV2K_train_HR.zip  DIV2K_valid_HR.zip\n```\nunzip 명령어로 압축해제.\n설치되어있지 않은 경우 `sudo apt install unzip`\n\n`unzip DIV2K_train_HR.zip`\n`unzip DIV2K_valid_HR.zip`\n```\n  inflating: DIV2K_train_HR/0544.png\n  inflating: DIV2K_train_HR/0416.png\n  inflating: DIV2K_train_HR/0295.png\n  inflating: DIV2K_train_HR/0538.png\n...\n  ```\n압축 해제되는 이미지 파일 하나하나가 표시된다.\n`ls` 명령어로 확인해보자\n```\nroot@7310e7ec2d72:~/SRGAN/data# ls\nDIV2K_train_HR  DIV2K_train_HR.zip  DIV2K_valid_HR  DIV2K_valid_HR.zip\n```\n압축 해제된 것을 확인할 수 있다.\n\n다시 SRGAN 폴더로 돌아가자.\n`cd ..`\ndataset을 넣어줬으니 train이 정상작동할 것이다.\n`python train.py`\n```\nroot@7310e7ec2d72:~/SRGAN# python train.py\n# generator parameters: 734219\n# discriminator parameters: 5215425\nDownloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /opt/ml/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n100%|████████████████████████████████████████████████████████████████████████████████| 528M/528M [00:08\u003c00:00, 63.1MB/s]\n[1/100] Loss_D: 0.8443 Loss_G: 0.0450 D(x): 0.5024 D(G(z)): 0.3312: 100%|███████████████| 13/13 [00:26\u003c00:00,  2.06s/it]\n[converting LR images to SR images] PSNR: 15.3047 dB SSIM: 0.3970: 100%|██████████████| 100/100 [00:33\u003c00:00,  3.03it/s]\n[saving training results]: 100%|████████████████████████████████████████████████████████| 20/20 [00:20\u003c00:00,  1.01s/it]\n[2/100] Loss_D: 0.8718 Loss_G: 0.0213 D(x): 0.4618 D(G(z)): 0.2530: 100%|███████████████| 13/13 [00:26\u003c00:00,  2.04s/it]\n[converting LR images to SR images] PSNR: 14.3030 dB SSIM: 0.3909: 100%|██████████████| 100/100 [00:32\u003c00:00,  3.03it/s]\n[saving training results]: 100%|████████████████████████████████████████████████████████| 20/20 [00:20\u003c00:00,  1.05s/it]\n[3/100] Loss_D: 0.8175 Loss_G: 0.0186 D(x): 0.4169 D(G(z)): 0.2086: 100%|███████████████| 13/13 [00:26\u003c00:00,  2.04s/it]\n[converting LR images to SR images] PSNR: 18.2039 dB SSIM: 0.5148: 100%|██████████████| 100/100 [00:34\u003c00:00,  2.91it/s]\n[saving training results]: 100%|████████████████████████████████████████████████████████| 20/20 [00:20\u003c00:00,  1.03s/it]\n[4/100] Loss_D: 0.4785 Loss_G: 0.0176 D(x): 0.6804 D(G(z)): 0.1720:  31%|████▉           | 4/13 [00:08\u003c00:25,  2.88s/it][4/100] Loss_D: 0.5887 Loss_G: 0.0185 D(x): 0.6745 D(G(z)): 0.2311: 100%|███████████████| 13/13 [00:26\u003c00:00,  2.04s/it]\n[converting LR images to SR images] PSNR: 18.2836 dB SSIM: 0.5135:  39%|█████▊         | 39/100 [00:13\u003c00:22,  2.70it/s][converting LR images to SR images] PSNR: 18.4853 dB SSIM: 0.5229:  75%|███████████▎   | 75/100 [00:25\u003c00:07,  3.26it/s]\n```\n정상적으로 실행된다.\n\n","short_description":"SRGAN을 무작정 실행시켜보자","is_markdown":true,"is_private":false,"is_temp":false,"thumbnail":"https://velog.velcdn.com/images/hyun-wle/post/cb14b237-f1e0-439a-bad6-522744806aed/image.png","comments_count":0,"url_slug":"SRGAN-따라하기","likes":1,"liked":false,"user":{"type":"id","generated":false,"id":"User:9958420e-d27d-4f31-92eb-ab6bb8f5f2ff","typename":"User"},"comments":[],"series":null,"linked_posts":{"type":"id","generated":true,"id":"$Post:810b29de-7c97-4013-be91-a9738a1e0eff.linked_posts","typename":"LinkedPosts"},"__typename":"Post"},"User:9958420e-d27d-4f31-92eb-ab6bb8f5f2ff":{"id":"9958420e-d27d-4f31-92eb-ab6bb8f5f2ff","username":"hyun-wle","profile":{"type":"id","generated":false,"id":"UserProfile:a6fe6c2c-eb58-4469-accb-90d94735f3b1","typename":"UserProfile"},"velog_config":{"type":"id","generated":true,"id":"$User:9958420e-d27d-4f31-92eb-ab6bb8f5f2ff.velog_config","typename":"VelogConfig"},"__typename":"User"},"UserProfile:a6fe6c2c-eb58-4469-accb-90d94735f3b1":{"id":"a6fe6c2c-eb58-4469-accb-90d94735f3b1","display_name":"Least-Efficient","thumbnail":"https://images.velog.io/images/hyun-wle/profile/3d55e02f-79ad-4276-865c-8cfab281bd49/KakaoTalk_20220117_170906404.jpg","short_bio":"","profile_links":{"type":"json","json":{}},"__typename":"UserProfile"},"$User:9958420e-d27d-4f31-92eb-ab6bb8f5f2ff.velog_config":{"title":"LoE","__typename":"VelogConfig"},"Post:3998af45-75f8-4af7-8c46-34176c3f8c20":{"id":"3998af45-75f8-4af7-8c46-34176c3f8c20","title":"깃허브2","url_slug":"깃허브2","user":{"type":"id","generated":false,"id":"User:9958420e-d27d-4f31-92eb-ab6bb8f5f2ff","typename":"User"},"__typename":"Post"},"$Post:810b29de-7c97-4013-be91-a9738a1e0eff.linked_posts":{"previous":{"type":"id","generated":false,"id":"Post:3998af45-75f8-4af7-8c46-34176c3f8c20","typename":"Post"},"next":{"type":"id","ge…
